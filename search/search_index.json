{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Account Creation Create an account with the cluster at Account Console , or ask an administrator to create one on your behalf. Once an account has been created, it will not have access to any resources until granted such by an administrator. JupyterHub (FPGA, GPU) Access GPUs and FPGAs can be accessed via JupyterHub while connected to the UA network. Connection can be either via an Ethernet cable in a UA-building, or via the UA VPN . Note that if a proxy is enabled in your browser settings, it must be disabled. There are two notebook options available that support FPGAs: MATLAB with FPGA (1x Alveo U200) MATLAB without FPGA Currently, the cluster has 3 FPGAs installed. If all FPGAs are in use, your notebook will not launch. Each FPGA notebook is provided with MATLAB Online, as well as VNC access to the Desktop version via the \"Desktop\" option. To launch MATLAB in the Desktop option, from a terminal window, run DISPLAY=:1 /opt/matlab/bin/matlab Additionally, there are four notebooks that support GPUs: PyTorch with GPU PyTorch without GPU TensorFlow with GPU TensorFlow without GPU There is currently 1 GPU in the cluster: 1x NVIDIA A100 (40GB) 2x NVIDIA A100 (80GB) (Temporarily Removed) Installation of HDL Toolbox / Coder FPGA Support Libraries During the extension install process, the Xilinx support libraries are written to the $HOME/Documents folder, which is overwritten by your user data volume. As such, to enable the support packages, run the following command while launching a MatLab environment: wget https://www.mathworks.com/mpm/glnxa64/mpm \\ && chmod +x mpm \\ && ./mpm install \\ --release=r2024b \\ --destination=/opt/matlab \\ --products Deep_Learning_HDL_Toolbox_Support_Package_for_Xilinx_FPGA_and_SoC_Devices HDL_Coder_Support_Package_for_Xilinx_FPGA_and_SoC_Devices This only needs to be ran once per-user. Docker Registry When deploying images to run on the cluster, the cluster's Docker Registry should be used. Note that when authenticating the registry via the command-line, the password prompt should be filled with a token generated from the registry's web GUI, not your actual password.","title":"Getting Started"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#account-creation","text":"Create an account with the cluster at Account Console , or ask an administrator to create one on your behalf. Once an account has been created, it will not have access to any resources until granted such by an administrator.","title":"Account Creation"},{"location":"#jupyterhub-fpga-gpu-access","text":"GPUs and FPGAs can be accessed via JupyterHub while connected to the UA network. Connection can be either via an Ethernet cable in a UA-building, or via the UA VPN . Note that if a proxy is enabled in your browser settings, it must be disabled. There are two notebook options available that support FPGAs: MATLAB with FPGA (1x Alveo U200) MATLAB without FPGA Currently, the cluster has 3 FPGAs installed. If all FPGAs are in use, your notebook will not launch. Each FPGA notebook is provided with MATLAB Online, as well as VNC access to the Desktop version via the \"Desktop\" option. To launch MATLAB in the Desktop option, from a terminal window, run DISPLAY=:1 /opt/matlab/bin/matlab Additionally, there are four notebooks that support GPUs: PyTorch with GPU PyTorch without GPU TensorFlow with GPU TensorFlow without GPU There is currently 1 GPU in the cluster: 1x NVIDIA A100 (40GB) 2x NVIDIA A100 (80GB) (Temporarily Removed)","title":"JupyterHub (FPGA, GPU) Access"},{"location":"#installation-of-hdl-toolbox-coder-fpga-support-libraries","text":"During the extension install process, the Xilinx support libraries are written to the $HOME/Documents folder, which is overwritten by your user data volume. As such, to enable the support packages, run the following command while launching a MatLab environment: wget https://www.mathworks.com/mpm/glnxa64/mpm \\ && chmod +x mpm \\ && ./mpm install \\ --release=r2024b \\ --destination=/opt/matlab \\ --products Deep_Learning_HDL_Toolbox_Support_Package_for_Xilinx_FPGA_and_SoC_Devices HDL_Coder_Support_Package_for_Xilinx_FPGA_and_SoC_Devices This only needs to be ran once per-user.","title":"Installation of HDL Toolbox / Coder FPGA Support Libraries"},{"location":"#docker-registry","text":"When deploying images to run on the cluster, the cluster's Docker Registry should be used. Note that when authenticating the registry via the command-line, the password prompt should be filled with a token generated from the registry's web GUI, not your actual password.","title":"Docker Registry"},{"location":"policies/","text":"Policies No Refunds We don't promise anything. While we try to be secure, and while we try not to lose all your data, neither of those are things we can actually deliver on with our current staffing. We also don't take any sort of backups - in other words, if we get ransomeware'd, you accidentally rm -rf /bighome/* , or you give your password to someone who doesn't like you, we can't help. No Privacy Though this falls under the above, we also cannot guarantee the privacy nor integrity of your data. It could be read, modified, or deleted by anyone on the network at any time. Personal Use Don't give someone a reason to stop you and nobody will care. Note that if you're running a personal notebook and consuming GPU/FPGA resources, someone may kill it.","title":"Policies"},{"location":"policies/#policies","text":"","title":"Policies"},{"location":"policies/#no-refunds","text":"We don't promise anything. While we try to be secure, and while we try not to lose all your data, neither of those are things we can actually deliver on with our current staffing. We also don't take any sort of backups - in other words, if we get ransomeware'd, you accidentally rm -rf /bighome/* , or you give your password to someone who doesn't like you, we can't help.","title":"No Refunds"},{"location":"policies/#no-privacy","text":"Though this falls under the above, we also cannot guarantee the privacy nor integrity of your data. It could be read, modified, or deleted by anyone on the network at any time.","title":"No Privacy"},{"location":"policies/#personal-use","text":"Don't give someone a reason to stop you and nobody will care. Note that if you're running a personal notebook and consuming GPU/FPGA resources, someone may kill it.","title":"Personal Use"},{"location":"slurm/","text":"HPC with Slurm Deprecated Slurm access is deprecated. If there are files in your /home/ , /bighome , or /scratch directories, please reach out to an administrator for access. Login Simply having an account with HPC permissions is not sufficient to login to the cluster. To do so, please request access via a system administrator. Then, login to the cluster with the following command: ssh username@10.116.25.113 # The static IP is in-place due to issues with OIT DNS # The actual command is below: # ssh username@hpc.cms.physics.ua.edu Resources Data Storage We provide three networked volumes for your convenience: /home/your-name is the generic home directory. Assume that space is limited. /bighome/your-name is based on HDDs, and intended for large objects. While we don't set per-user quotas, we do have an overall cluster limit. /scratch/your-name is based on SSDs. Note that this directory should not be used for long-term storage , and will be wiped regularly . Note that your bighome and scratch directories are availible under $BIGHOME and $SCRATCH . Network Ports Ports 30000-30500 are exposed on all nodes. Using Jupyter We will create a Jupyterlab setup in a Conda environment. For the purposes of instruction, we will install Miniforge and create a sample Conda environment. # Install Miniforge curl https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh chmod +x Miniforge3-Linux-x86_64.sh ./Miniforge3-Linux-x86_64.sh # Create Sample Environment conda create --name JupyterTutorial # Activate the sample environment conda activate JupyterTutorial # Install Jupyter conda install jupyter Next, we will create a script to launch a Jupyter environment, to be executed via srun . Note that the port below, 30000 , may not be availible on your particular node. See the Network Ports section above for potentially availible ports. echo '#!/bin/bash conda activate JupyterTutorial jupyter-lab --no-browser --ip 0.0.0.0 --port 30000' > ./jupyter.sh chmod +x ~/jupyter.sh Next, use srun to launch the notebook. # Start a notebook with 2 CPUs and 16GB of RAM srun -t 6-09:59:59 --cpus-per-task=2 --ntasks=1 --mem-per-cpu=8G --pty ~/jupyter.sh # Start a notebook with 16 CPUs, 64GB of RAM, and an A100 (80GB) srun -t 6-09:59:59 --cpus-per-task=16 --ntasks=1 --mem-per-cpu=4G --gres=gpu:a100-80gb:1 --pty ~/jupyter.sh # Start a notebook with 32 CPUs, 64GB of RAM, and 2 GPUs srun -t 6-09:59:59 --cpus-per-task=32 --ntasks=1 --mem-per-cpu=2G --gres=gpu:2 --pty ~/jupyter.sh # Start a notebook with 32 CPUs, 64GB of RAM, and an A100 (40GB) srun -t 6-09:59:59 --cpus-per-task=32 --ntasks=1 --mem-per-cpu=2G --gres=gpu:a100-40gb:1 --pty ~/jupyter.sh","title":"HPC with Slurm"},{"location":"slurm/#hpc-with-slurm","text":"","title":"HPC with Slurm"},{"location":"slurm/#deprecated","text":"Slurm access is deprecated. If there are files in your /home/ , /bighome , or /scratch directories, please reach out to an administrator for access.","title":"Deprecated"},{"location":"slurm/#login","text":"Simply having an account with HPC permissions is not sufficient to login to the cluster. To do so, please request access via a system administrator. Then, login to the cluster with the following command: ssh username@10.116.25.113 # The static IP is in-place due to issues with OIT DNS # The actual command is below: # ssh username@hpc.cms.physics.ua.edu","title":"Login"},{"location":"slurm/#resources","text":"","title":"Resources"},{"location":"slurm/#data-storage","text":"We provide three networked volumes for your convenience: /home/your-name is the generic home directory. Assume that space is limited. /bighome/your-name is based on HDDs, and intended for large objects. While we don't set per-user quotas, we do have an overall cluster limit. /scratch/your-name is based on SSDs. Note that this directory should not be used for long-term storage , and will be wiped regularly . Note that your bighome and scratch directories are availible under $BIGHOME and $SCRATCH .","title":"Data Storage"},{"location":"slurm/#network-ports","text":"Ports 30000-30500 are exposed on all nodes.","title":"Network Ports"},{"location":"slurm/#using-jupyter","text":"We will create a Jupyterlab setup in a Conda environment. For the purposes of instruction, we will install Miniforge and create a sample Conda environment. # Install Miniforge curl https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh chmod +x Miniforge3-Linux-x86_64.sh ./Miniforge3-Linux-x86_64.sh # Create Sample Environment conda create --name JupyterTutorial # Activate the sample environment conda activate JupyterTutorial # Install Jupyter conda install jupyter Next, we will create a script to launch a Jupyter environment, to be executed via srun . Note that the port below, 30000 , may not be availible on your particular node. See the Network Ports section above for potentially availible ports. echo '#!/bin/bash conda activate JupyterTutorial jupyter-lab --no-browser --ip 0.0.0.0 --port 30000' > ./jupyter.sh chmod +x ~/jupyter.sh Next, use srun to launch the notebook. # Start a notebook with 2 CPUs and 16GB of RAM srun -t 6-09:59:59 --cpus-per-task=2 --ntasks=1 --mem-per-cpu=8G --pty ~/jupyter.sh # Start a notebook with 16 CPUs, 64GB of RAM, and an A100 (80GB) srun -t 6-09:59:59 --cpus-per-task=16 --ntasks=1 --mem-per-cpu=4G --gres=gpu:a100-80gb:1 --pty ~/jupyter.sh # Start a notebook with 32 CPUs, 64GB of RAM, and 2 GPUs srun -t 6-09:59:59 --cpus-per-task=32 --ntasks=1 --mem-per-cpu=2G --gres=gpu:2 --pty ~/jupyter.sh # Start a notebook with 32 CPUs, 64GB of RAM, and an A100 (40GB) srun -t 6-09:59:59 --cpus-per-task=32 --ntasks=1 --mem-per-cpu=2G --gres=gpu:a100-40gb:1 --pty ~/jupyter.sh","title":"Using Jupyter"},{"location":"administration/01-user-management/","text":"User Management User Creation Users should be able to sign up via the Account Console . If not, add them yourselves via the Admin Console . Once a user has registered, in the Admin Console, you may assign them a temporary password if they have not set one on their own. Additionally, a user should be assigned the HPC, Admin, or Sysadmin roles. The HPC role allows access to all our cluster's services. The Admin role (you) allows access to administrator features of apps (eg. opening other people's jupyter environments, editing other docker libraries), as well as the user admin console itself. The Sysadmin role allows access to administrate the entire cluster, and is generally something you shouldn't touch. You can assign it to yourself or others in an emergency if nessecary.","title":"User Management"},{"location":"administration/01-user-management/#user-management","text":"","title":"User Management"},{"location":"administration/01-user-management/#user-creation","text":"Users should be able to sign up via the Account Console . If not, add them yourselves via the Admin Console . Once a user has registered, in the Admin Console, you may assign them a temporary password if they have not set one on their own. Additionally, a user should be assigned the HPC, Admin, or Sysadmin roles. The HPC role allows access to all our cluster's services. The Admin role (you) allows access to administrator features of apps (eg. opening other people's jupyter environments, editing other docker libraries), as well as the user admin console itself. The Sysadmin role allows access to administrate the entire cluster, and is generally something you shouldn't touch. You can assign it to yourself or others in an emergency if nessecary.","title":"User Creation"},{"location":"sytem-administration/ldap-kerberos/","text":"Account Control LDAP OLC Access Rules Query why we want read by * ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=%sysadmin,ou=sudoers,dc=cms,dc=physics,dc=ua,dc=edu ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config '(olcDatabase={2}mdb)' olcAccess ldapmodify -Q -Y EXTERNAL -H ldapi:///<<EOF dn: olcDatabase={2}mdb,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=1001,cn=peercred,cn=external,cn=auth manage by * break olcAccess: {1}to attrs=userPassword,shadowLastChange by self write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by anonymous auth by * none olcAccess: {2}to * by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by self read by * read EOF SSH Keys (Run in OpenLDAP) Source: Link ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF dn: cn=openssh-lpk-openldap,cn=schema,cn=config objectClass: olcSchemaConfig olcAttributeTypes: {0}( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' D ESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6. 1.4.1.1466.115.121.1.40 ) olcObjectClasses: {0}( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' DE SC 'MANDATORY: OpenSSH LPK objectclass' SUP top AUXILIARY MUST ( sshPublicK ey $ uid ) ) EOF Kerberos Machine Initialization - Create Keytab On the Admin Machine HOST=rose kubectl -n kerberos exec -it $(kubectl -n kerberos get pod -l \"component=kadmin\" -o jsonpath='{.items[0].metadata.name}') -- bash -c \"rm /etc/krb5.keytab; kadmin.local addprinc -randkey host/$HOST; kadmin.local ktadd -k /etc/krb5.keytab host/$HOST; base64 /etc/krb5.keytab\" On the target: base64 -d | sudo tee /etc/krb5.keytab sudo chmod 600 /etc/krb5.keytab Schemas (run in openldap) # Text from https://raw.githubusercontent.com/krb5/krb5/master/src/plugins/kdb/ldap/libkdb_ldap/kerberos.openldap.ldif -o kerberos.openldap.ldif ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF EOF olcAccess (run in openldap) ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \"(|(cn=config)(olcDatabase={2}mdb))\" ldapmodify -Q -Y EXTERNAL -H ldapi:///<<EOF dn: olcDatabase={2}mdb,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=1001,cn=peercred,cn=external,cn=auth manage by * break olcAccess: {1}to attrs=userPassword,shadowLastChange by self write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by anonymous auth by * none olcAccess: {2}to attrs=krbPrincipalKey by anonymous auth by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by self write by * none olcAccess: {3}to dn.subtree=\"cn=krbContainer,dc=cms,dc=physics,dc=ua,dc=edu\" by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * none olcAccess: {4}to dn.subtree=\"ou=users,dc=cms,dc=physics,dc=ua,dc=edu\" by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * read olcAccess: {5}to * by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * read EOF Admin Accounts (run in openldap) ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF dn: uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu uid: kdc-service objectClass: account objectClass: simpleSecurityObject userPassword: {CRYPT}x description: Account used for the Kerberos KDC dn: uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu uid: kadmin-service objectClass: account objectClass: simpleSecurityObject userPassword: {CRYPT}x description: Account used for the Kerberos Admin server EOF ldappasswd -Q -Y EXTERNAL -H ldapi:/// uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu -S ldappasswd -Q -Y EXTERNAL -H ldapi:/// uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu -S Create Realm (run in kerberos) kdb5_ldap_util create -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu -subtrees ou=users,dc=cms,dc=physics,dc=ua,dc=edu -r CMS.PHYSICS.UA.EDU -s -sf /etc/krb5kdc/stash2 -H ldap://openldap.openldap.svc.cluster.local base64 /etc/krb5kdc/stash2 -w0 # for secret kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu -H ldap://openldap.openldap.svc.cluster.local view -r CMS.PHYSICS.UA.EDU kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu stashsrvpw -f /etc/krb5kdc/service2.keyfile uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu stashsrvpw -f /etc/krb5kdc/service2.keyfile uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu base64 /etc/krb5kdc/service2.keyfile -w0 # for secret","title":"Account Control"},{"location":"sytem-administration/ldap-kerberos/#account-control","text":"","title":"Account Control"},{"location":"sytem-administration/ldap-kerberos/#ldap","text":"","title":"LDAP"},{"location":"sytem-administration/ldap-kerberos/#olc-access-rules","text":"Query why we want read by * ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=%sysadmin,ou=sudoers,dc=cms,dc=physics,dc=ua,dc=edu ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config '(olcDatabase={2}mdb)' olcAccess ldapmodify -Q -Y EXTERNAL -H ldapi:///<<EOF dn: olcDatabase={2}mdb,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=1001,cn=peercred,cn=external,cn=auth manage by * break olcAccess: {1}to attrs=userPassword,shadowLastChange by self write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by anonymous auth by * none olcAccess: {2}to * by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by self read by * read EOF","title":"OLC Access Rules"},{"location":"sytem-administration/ldap-kerberos/#ssh-keys-run-in-openldap","text":"Source: Link ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF dn: cn=openssh-lpk-openldap,cn=schema,cn=config objectClass: olcSchemaConfig olcAttributeTypes: {0}( 1.3.6.1.4.1.24552.500.1.1.1.13 NAME 'sshPublicKey' D ESC 'MANDATORY: OpenSSH Public key' EQUALITY octetStringMatch SYNTAX 1.3.6. 1.4.1.1466.115.121.1.40 ) olcObjectClasses: {0}( 1.3.6.1.4.1.24552.500.1.1.2.0 NAME 'ldapPublicKey' DE SC 'MANDATORY: OpenSSH LPK objectclass' SUP top AUXILIARY MUST ( sshPublicK ey $ uid ) ) EOF","title":"SSH Keys (Run in OpenLDAP)"},{"location":"sytem-administration/ldap-kerberos/#kerberos","text":"","title":"Kerberos"},{"location":"sytem-administration/ldap-kerberos/#machine-initialization-create-keytab","text":"On the Admin Machine HOST=rose kubectl -n kerberos exec -it $(kubectl -n kerberos get pod -l \"component=kadmin\" -o jsonpath='{.items[0].metadata.name}') -- bash -c \"rm /etc/krb5.keytab; kadmin.local addprinc -randkey host/$HOST; kadmin.local ktadd -k /etc/krb5.keytab host/$HOST; base64 /etc/krb5.keytab\" On the target: base64 -d | sudo tee /etc/krb5.keytab sudo chmod 600 /etc/krb5.keytab","title":"Machine Initialization - Create Keytab"},{"location":"sytem-administration/ldap-kerberos/#schemas-run-in-openldap","text":"# Text from https://raw.githubusercontent.com/krb5/krb5/master/src/plugins/kdb/ldap/libkdb_ldap/kerberos.openldap.ldif -o kerberos.openldap.ldif ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF EOF","title":"Schemas (run in openldap)"},{"location":"sytem-administration/ldap-kerberos/#olcaccess-run-in-openldap","text":"ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \"(|(cn=config)(olcDatabase={2}mdb))\" ldapmodify -Q -Y EXTERNAL -H ldapi:///<<EOF dn: olcDatabase={2}mdb,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.exact=gidNumber=0+uidNumber=1001,cn=peercred,cn=external,cn=auth manage by * break olcAccess: {1}to attrs=userPassword,shadowLastChange by self write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by anonymous auth by * none olcAccess: {2}to attrs=krbPrincipalKey by anonymous auth by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by self write by * none olcAccess: {3}to dn.subtree=\"cn=krbContainer,dc=cms,dc=physics,dc=ua,dc=edu\" by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * none olcAccess: {4}to dn.subtree=\"ou=users,dc=cms,dc=physics,dc=ua,dc=edu\" by dn.exact=\"uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu\" read by dn.exact=\"uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu\" write by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * read olcAccess: {5}to * by dn=\"cn=admin,dc=cms,dc=physics,dc=ua,dc=edu\" write by * read EOF","title":"olcAccess (run in openldap)"},{"location":"sytem-administration/ldap-kerberos/#admin-accounts-run-in-openldap","text":"ldapadd -Q -Y EXTERNAL -H ldapi:///<<EOF dn: uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu uid: kdc-service objectClass: account objectClass: simpleSecurityObject userPassword: {CRYPT}x description: Account used for the Kerberos KDC dn: uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu uid: kadmin-service objectClass: account objectClass: simpleSecurityObject userPassword: {CRYPT}x description: Account used for the Kerberos Admin server EOF ldappasswd -Q -Y EXTERNAL -H ldapi:/// uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu -S ldappasswd -Q -Y EXTERNAL -H ldapi:/// uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu -S","title":"Admin Accounts (run in openldap)"},{"location":"sytem-administration/ldap-kerberos/#create-realm-run-in-kerberos","text":"kdb5_ldap_util create -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu -subtrees ou=users,dc=cms,dc=physics,dc=ua,dc=edu -r CMS.PHYSICS.UA.EDU -s -sf /etc/krb5kdc/stash2 -H ldap://openldap.openldap.svc.cluster.local base64 /etc/krb5kdc/stash2 -w0 # for secret kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu -H ldap://openldap.openldap.svc.cluster.local view -r CMS.PHYSICS.UA.EDU kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu stashsrvpw -f /etc/krb5kdc/service2.keyfile uid=kdc-service,dc=cms,dc=physics,dc=ua,dc=edu kdb5_ldap_util -D cn=admin,dc=cms,dc=physics,dc=ua,dc=edu stashsrvpw -f /etc/krb5kdc/service2.keyfile uid=kadmin-service,dc=cms,dc=physics,dc=ua,dc=edu base64 /etc/krb5kdc/service2.keyfile -w0 # for secret","title":"Create Realm (run in kerberos)"},{"location":"sytem-administration/storage/","text":"Rook Debugging Access Ceph Toolbox kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- bash Remove / Move OSD kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 kubectl -n rook-ceph-cluster scale deployment rook-ceph-osd-<ID> --replicas=0 kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- ceph osd down osd.<ID> kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- ceph osd purge <ID> --yes-i-really-mean-it kubectl delete deployment -n rook-ceph-cluster rook-ceph-osd-<ID> kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1 Fix Mgr kubectl -n rook-ceph-cluster rollout restart deployment/rook-ceph-mgr-a","title":"Rook Debugging"},{"location":"sytem-administration/storage/#rook-debugging","text":"","title":"Rook Debugging"},{"location":"sytem-administration/storage/#access-ceph-toolbox","text":"kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- bash","title":"Access Ceph Toolbox"},{"location":"sytem-administration/storage/#remove-move-osd","text":"kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 kubectl -n rook-ceph-cluster scale deployment rook-ceph-osd-<ID> --replicas=0 kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- ceph osd down osd.<ID> kubectl -n rook-ceph-cluster exec -it $(kubectl -n rook-ceph-cluster get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') -- ceph osd purge <ID> --yes-i-really-mean-it kubectl delete deployment -n rook-ceph-cluster rook-ceph-osd-<ID> kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1","title":"Remove / Move OSD"},{"location":"sytem-administration/storage/#fix-mgr","text":"kubectl -n rook-ceph-cluster rollout restart deployment/rook-ceph-mgr-a","title":"Fix Mgr"}]}